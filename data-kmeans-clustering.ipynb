{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e29bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e760a31",
   "metadata": {},
   "source": [
    "# K-Means Clustering with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a271a6",
   "metadata": {},
   "source": [
    "__K-Means Clustering__ is a popular unsupervised machine learning algorithm. K-Means clustering is used to find intrinsic groups within the unlabeled dataset and draw inferences from them. In this kernel, we'll implement K-Means clustering to find intrinsic groups within the dataset that display the same __status_type__ behavior. The __status_type__ behavior variable consists of posts of a different nature (video, photos, statuses and links).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c421f",
   "metadata": {},
   "source": [
    "# Introduction to K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ba519",
   "metadata": {},
   "source": [
    "Machine learning algorithms can be broadly classified into two categories - supervised and unsupervised learning. In unsupervised learning, there is no target variable. The dataset only has input variables which describe the data.\n",
    "\n",
    "\n",
    "__K-Means clustering__ is an unsupervised learning algorithm. It's used when we have unlabeled data which is data without defined categories or groups. The algorithm follows an easy or simple way to classify a given data set through a certain number of clusters, fixed apriori. K-Means algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa87514",
   "metadata": {},
   "source": [
    "## Applications of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7ace8e",
   "metadata": {},
   "source": [
    "- K-Means clustering is widely used for many applications which include:\n",
    "\n",
    "    - Image segmentation\n",
    "    - Customer segmentation\n",
    "    - Species clustering\n",
    "    - Anomaly detection\n",
    "    - Clustering languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9bf0b",
   "metadata": {},
   "source": [
    "## K-Means Clustering intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a927c79a",
   "metadata": {},
   "source": [
    "- K-Means clustering is used to find intrinsic groups within the unlabeled dataset and draw inferences from them. It's based on centroid-based clustering.\n",
    "\n",
    "\n",
    "- __Centroid__: a centroid is a data point at the center of a cluster. In centroid-based clustering, clusters are represented by a centroid. It is an iterative algorithm in which the notion of similarity is derived by how close a data point is to the centroid of the cluster. K-Means clustering algorithm uses an iterative procedure to deliver a final result. The algorithm requires a number of clusters K and the dataset as input. The dataset is a collection of features for each data point. The algorithm starts with initial estimates for the K centroids. The algorithm then iterates between two steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e4da6",
   "metadata": {},
   "source": [
    "### Data assignment step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd1dc9c",
   "metadata": {},
   "source": [
    "Each centroid defines one of the clusters. In this step, each data point is assigned to its nearest centroid, which is based on the squared Euclidean distance. So, if ci is the collection of centroids in set C, then each data point is assigned to a cluster based on minimum Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac08fa1",
   "metadata": {},
   "source": [
    "### Centroid update step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea81d17",
   "metadata": {},
   "source": [
    "In this step, the centroids are recomputed and updated. This is done by taking the mean of all data points assigned to that centroid's cluster.\n",
    "\n",
    "\n",
    "The algorithm then iterates between step 1 and step 2 until a stopping criteria is met. Stopping criteria means no data points change the clusters, the sum of the distances is minimized or some maximum number of iterations is reached. This algorithm is guaranteed to converge to a result. The result may be a local optimum meaning that assessing more than one run of the algorithm with randomized starting centroids may give a better outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d81545",
   "metadata": {},
   "source": [
    "## Choosing the value of K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67759a4d",
   "metadata": {},
   "source": [
    "The K-Means algorithm depends upon finding the number of clusters and data labels for a pre-defined value of K. To find the number of clusters in the data, we need to run the K-Means clustering algorithm for different values of K and compare the results. So, the performance of K-Means algorithm depends upon the value of K. We should choose the optimal value of K that gives us the best performance. There are different techniques available to find the optimal value of K. The most common technique is the __elbow method__ which is described below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6f45cd",
   "metadata": {},
   "source": [
    "## Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df52cb2",
   "metadata": {},
   "source": [
    "The elbow method is used to determine the optimal number of clusters in K-Means clustering. The elbow method plots the value of the cost function produced by different values of K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00191bd",
   "metadata": {},
   "source": [
    "If K increases, average distortion will decrease. Then each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as K increases. The value of K at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83240a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
